# References

Curated, evolving list of sources for the Valerie decoder-only transformer implementation.  
**Last updated:** 2025-11-12

## Academic Papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
  Ashish Vaswani et al., 2017  
  `[vaswani2017-attention]`

- [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)  
  Google Deepmind, 2022  
  `[deepmind2022-formal-transformers]`

- [The Backpropagation Algorithm for a Math Student](https://arxiv.org/abs/2301.09977)  
  Damadi et al., 2023  
  `[damadi2023-backprop]`

- [A Theoretical Framework for Back-Propagation](https://new.math.uiuc.edu/MathMLseminar/seminarPapers/LeCunBackprop1988.pdf)  
  Yann LeCun, 1988  
  `[lecun1988-backprop]`

- [Learning Representations by Back-Propagating Errors](https://www.nature.com/articles/323533a0)  
  Geoffrey Hinton, 1986  
  _Proprietary_  
  `[hinton1986-backprop]`

- [Better Language Models and Their Implications](https://openai.com/index/better-language-models/)  
  Sutskever et al., 2019  
  _MIT License_  
  `[sutskever2019-better-lm]`

- [On the Difficulty of Training Recurrent Neural Networks](https://arxiv.org/abs/1211.5063)  
  Bengio et al., 2012  
  `[bengio2012-difficulty-rnn]`

- [Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors (Dropout)](https://arxiv.org/abs/1207.0580)  
  Hinton et al., 2012  
  `[hinton2012-dropout]`

- [Egalitarian Language Representation in Language Models: It All Begins with Tokenizers](https://arxiv.org/abs/2409.11501)  
  Menan Velayuthan, Kengatharaiyer Sarveswaran, 2024  
  `[velayuthan2024-egalitarian-tokenizers]`

- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)  
  Mikolov et al., 2016  
  `[mikolov2016-subword]`

- [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)  
  Micikevicius et al., 2022  
  `[micikevicius2022-fp8]`

- [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)  
  Ainslie et al., 2023  
  `[ainslie2023-gqa]`

- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He Initialization)](https://arxiv.org/abs/1502.01852)  
  He et al., 2015  
  `[he2015-init]`

- [IEEE Standard for Floating-Point Arithmetic](https://standards.ieee.org/ieee/754/6210/)  
  IEEE, 2019  
  _Proprietary_  
  `[ieee2019-float]`

- [On Weight Initialization in Deep Neural Networks](https://arxiv.org/abs/1704.08863)  
  Kumar, 2017  
  `[kumar2017-weight-init]`

- [Random Number Generators: Good Ones Are Hard to Find](https://dl.acm.org/doi/10.1145/63039.63042)  
  Park & Miller, 1988  
  _Publicly accessible_  
  `[parkmiller1988-lcg]`

- [Mersenne Twister: A 623-dimensionally Equidistributed Uniform Pseudo-Random Number Generator](https://dl.acm.org/doi/10.1145/272991.272995)  
  Matsumoto & Nishimura, 1998  
  _Publicly accessible_  
  `[matsumoto1998-mt]`

- [Microscaling Floating Point Formats for Large Language Models](https://arxiv.org/abs/2510.01863)  
  Cococcioni et al., 2025  
  `[cococcioni2025-microscaling]`

- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)  
  Micikevicius et al., 2017  
  `[micikevicius2017-mixed-precision]`

- [On the Importance of Initialization and Momentum in Deep Learning](https://proceedings.mlr.press/v28/sutskever13.html)  
  Sutskever et al., 2013  
  `[sutskever2013-momentum]`

- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)  
  Sennrich et al., 2015  
  `[sennrich2015-nmt-subword]`

- [Deep Residual Learning for Image Recognition (Residual Connections)](https://arxiv.org/abs/1512.03385)  
  He et al., 2015  
  `[he2015-residual]`

- [Reversed Attention: On The Gradient Descent Of Attention Layers In GPT](https://arxiv.org/abs/2412.17019)  
  Katz & Wolf, 2024  
  `[katz2024-reverse-attn]`

- [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)  
  Zhang & Sennrich, 2019  
  `[zhang2019-rmsnorm]`

- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)  
  Su et al., 2021  
  `[su2021-roformer-rope]`

- [Searching for Activation Functions (Swish)](https://arxiv.org/abs/1710.05941)  
  Ramachandran et al., 2017  
  `[ramachandran2017-swish]`

- [Using the Output Embedding to Improve Language Models (Tied Embeddings)](https://arxiv.org/abs/1608.05859)  
  Press & Wolf, 2016  
  `[press2016-tied-embed]`

- [Understanding the Difficulty of Training Deep Feedforward Neural Networks (Xavier Initialization)](https://proceedings.mlr.press/v9/glorot10a.html)  
  Glorot & Bengio, 2010  
  `[glorot2010-xavier]`

## Web Articles

- [Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/)  
  Lei Mao, 2019  
  `[mao2019-bpe]`

- [Softmax for Neural Networks](https://brandonrohrer.com/softmax)  
  Brandon Rohrer, 2020  
  `[rohrer2020-softmax]`

- [CS231n: Deep Learning for Computer Vision](https://cs231n.github.io)  
  Stanford, 2024  
  _(see Linear Classification section)_  
  `[cs231n2024]`

- [The Softmax Function and Its Derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)  
  Eli Bendersky, 2016  
  `[bendersky2016-softmax]`

- [Understanding Transformers and GPT: An In-depth Overview](https://incml.github.io/2023/03/05/Transformer-GPT.html)  
  Hui Jiang, 2023  
  `[jiang2023-transformer-gpt]`

- [Xavier and Kaiming Initialization](https://pouannes.github.io/blog/initialization/)  
  Pierre Ouannes, 2019  
  `[ouannes2019-init]`

- [Regularizing Your Neural Networks](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html)  
  Kian Katanforoosh & Daniel Kunin, 2018  
  `[katanforoosh2018-regularization]`

## Textbooks

- [Elementary Calculus: An Infinitesimal Approach](https://people.math.wisc.edu/~hkeisler)  
  H. Jerome Keisler, 2024  
  _Creative Commons_  
  `[keisler2024-elementary-calculus]`

- [Single and Multivariable Calculus: Early Transcendentals](https://www.whitman.edu/mathematics/multivariable/)  
  Guichard & Keisler, 2025  
  _Creative Commons_  
  `[guichard2025-multivariable]`

- [Deep Learning](https://www.deeplearningbook.org/)  
  Goodfellow et al., 2016  
  _Proprietary, web-only_  
  `[goodfellow2016-deep-learning]`

- [Understanding Deep Learning](https://udlbook.github.io/udlbook/)  
  Simon J.D. Prince, 2023  
  _Creative Commons_  
  `[prince2023-udl]`

- [Discrete Mathematics: An Open Introduction (3rd Edition)](https://discrete.openmathbooks.org/dmoi3.html)  
  Oscar Levin, 2021  
  _Creative Commons_  
  `[levin2021-discrete]`

- [Understanding Linear Algebra](https://understandinglinearalgebra.org/home.html)  
  David Austin, 2023  
  _Creative Commons_  
  `[austin2023-ula]`

- [Linear Algebra: Done Right](https://link.springer.com/book/10.1007/978-3-031-41026-0)  
  Sheldon Axler, 2024  
  _Creative Commons_  
  `[axler2024-ladr]`

- [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://hastie.su.domains/ElemStatLearn/)  
  Hastie et al., 2017  
  _(Accessibility in progress)_  
  `[hastie2017-esl]`

- [Introduction to Probability, Statistics, and Random Processes](https://www.probabilitycourse.com/)  
  Hossein Pishro-Nik, 2014  
  _Proprietary, web-only_  
  `[pishronik2014-ipsrp]`

- [Digital Signals Theory](https://brianmcfee.net/)  
  Brian McFee, 2023  
  _Proprietary, web-only_  
  `[mcfee2023-dst]`

- [The Scientist & Engineer's Guide to Digital Signal Processing](https://www.dspguide.com)  
  Steven W. Smith, 1999  
  _Proprietary, web-only_  
  `[smith1999-dsp]`

## Open Educational Resources (OER) / Creative Commons

- [OpenStax: Math Textbooks](https://openstax.org/subjects/math) `[openstax-math]`
- [Open Textbook Library: Mathematics (UMN)](https://open.umn.edu/opentextbooks/subjects/mathematics) `[umn-math-open]`
- [LibreTexts: Mathematics](https://math.libretexts.org/)

## Wikipedia

- [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) `[wiki-normal]`
- [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) `[wiki-chain-rule]`
- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) `[wiki-logreg]`
- [One-hot](https://en.wikipedia.org/wiki/One-hot) `[wiki-onehot]`
- [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function) `[wiki-sigmoid]`
- [IEEE 754 (Floating-point Precision)](https://en.wikipedia.org/wiki/IEEE_754) `[wiki-ieee754]`

## Source Code

- [adriancable/qwen3.c](https://github.com/adriancable/qwen3.c) `[adriancable-qwen3c]`
- [karpathy/llama2.c](https://github.com/karpathy/llama2.c) `[karpathy-llama2c]`
- [karpathy/micrograd](https://github.com/karpathy/micrograd) `[karpathy-micrograd]`
- [ggerganov/llama.cpp](https://github.com/ggml-org/llama.cpp) `[ggerganov-llamacpp]`
- [QwenLM/Qwen3](https://github.com/QwenLM/Qwen3) `[QwenLM-Qwen3]`
- [openai/gpt-2](https://github.com/openai/gpt-2) `[openai-gpt-2]`

## Notes / TODO

- Reference only: public domain, creative commons, or FOSS-licensed materials.
- Avoid: proprietary, patented, or otherwise restricted materials where possible.
- Sift out the difference between private and public materials and substitute if needed.
- Expand with subcategories (Tokenization, Initialization, Regularization) as needed.
- Optionally map `[citation-key]` to usage in code/docs.
- Maintain deduplication and alphabetization.
- Digital signal processing is a subtopic of mathematics and engineering, not a standalone field for citation purposes. Kept here for thematic relevance.
