# References

Curated, evolving list of sources for the Valerie decoder-only transformer implementation.  
**Last updated:** 2025-11-12

## Academic Papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)  
  Ashish Vaswani et al., 2017  
  `[vaswani2017-attention]`

- [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238)  
  Google Deepmind, 2022  
  `[deepmind2022-formal-transformers]`

- [The Backpropagation Algorithm for a Math Student](https://arxiv.org/abs/2301.09977)  
  Damadi et al., 2023  
  `[damadi2023-backprop]`

- [A Theoretical Framework for Back-Propagation](https://new.math.uiuc.edu/MathMLseminar/seminarPapers/LeCunBackprop1988.pdf)  
  Yann LeCun, 1988  
  `[lecun1988-backprop]`

- [Learning Representations by Back-Propagating Errors](https://www.nature.com/articles/323533a0)  
  Geoffrey Hinton, 1986  
  _Proprietary_  
  `[hinton1986-backprop]`

- [Better Language Models and Their Implications](https://openai.com/index/better-language-models/)  
  Sutskever et al., 2019  
  _MIT License_  
  `[sutskever2019-better-lm]`

- [On the Difficulty of Training Recurrent Neural Networks](https://arxiv.org/abs/1211.5063)  
  Bengio et al., 2012  
  `[bengio2012-difficulty-rnn]`

- [Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors (Dropout)](https://arxiv.org/abs/1207.0580)  
  Hinton et al., 2012  
  `[hinton2012-dropout]`

- [Egalitarian Language Representation in Language Models: It All Begins with Tokenizers](https://arxiv.org/abs/2409.11501)  
  Menan Velayuthan, Kengatharaiyer Sarveswaran, 2024  
  `[velayuthan2024-egalitarian-tokenizers]`

- [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)  
  Mikolov et al., 2016  
  `[mikolov2016-subword]`

- [FP8 Formats for Deep Learning](https://arxiv.org/abs/2209.05433)  
  Micikevicius et al., 2022  
  `[micikevicius2022-fp8]`

- [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)  
  Ainslie et al., 2023  
  `[ainslie2023-gqa]`

- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He Initialization)](https://arxiv.org/abs/1502.01852)  
  He et al., 2015  
  `[he2015-init]`

- [IEEE Standard for Floating-Point Arithmetic](https://standards.ieee.org/ieee/754/6210/)  
  IEEE, 2019  
  _Proprietary_  
  `[ieee2019-float]`

- [On Weight Initialization in Deep Neural Networks](https://arxiv.org/abs/1704.08863)  
  Kumar, 2017  
  `[kumar2017-weight-init]`

- [Random Number Generators: Good Ones Are Hard to Find](https://dl.acm.org/doi/10.1145/63039.63042)  
  Park & Miller, 1988  
  _Publicly accessible_  
  `[parkmiller1988-lcg]`

- [Mersenne Twister: A 623-dimensionally Equidistributed Uniform Pseudo-Random Number Generator](https://dl.acm.org/doi/10.1145/272991.272995)  
  Matsumoto & Nishimura, 1998  
  _Publicly accessible_  
  `[matsumoto1998-mt]`

- [Microscaling Floating Point Formats for Large Language Models](https://arxiv.org/abs/2510.01863)  
  Cococcioni et al., 2025  
  `[cococcioni2025-microscaling]`

- [Mixed Precision Training](https://arxiv.org/abs/1710.03740)  
  Micikevicius et al., 2017  
  `[micikevicius2017-mixed-precision]`

- [On the Importance of Initialization and Momentum in Deep Learning](https://proceedings.mlr.press/v28/sutskever13.html)  
  Sutskever et al., 2013  
  `[sutskever2013-momentum]`

- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)  
  Sennrich et al., 2015  
  `[sennrich2015-nmt-subword]`

- [Deep Residual Learning for Image Recognition (Residual Connections)](https://arxiv.org/abs/1512.03385)  
  He et al., 2015  
  `[he2015-residual]`

- [Reversed Attention: On The Gradient Descent Of Attention Layers In GPT](https://arxiv.org/abs/2412.17019)  
  Katz & Wolf, 2024  
  `[katz2024-reverse-attn]`

- [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)  
  Zhang & Sennrich, 2019  
  `[zhang2019-rmsnorm]`

- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)  
  Su et al., 2021  
  `[su2021-roformer-rope]`

- [Searching for Activation Functions (Swish)](https://arxiv.org/abs/1710.05941)  
  Ramachandran et al., 2017  
  `[ramachandran2017-swish]`

- [Using the Output Embedding to Improve Language Models (Tied Embeddings)](https://arxiv.org/abs/1608.05859)  
  Press & Wolf, 2016  
  `[press2016-tied-embed]`

- [Understanding the Difficulty of Training Deep Feedforward Neural Networks (Xavier Initialization)](https://proceedings.mlr.press/v9/glorot10a.html)  
  Glorot & Bengio, 2010  
  `[glorot2010-xavier]`

## Web Articles

- [Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/)  
  Lei Mao, 2019  
  `[mao2019-bpe]`

- [Softmax for Neural Networks](https://brandonrohrer.com/softmax)  
  Brandon Rohrer, 2020  
  `[rohrer2020-softmax]`

- [CS231n: Deep Learning for Computer Vision](https://cs231n.github.io)  
  Stanford, 2024  
  _(see Linear Classification section)_  
  `[cs231n2024]`

- [The Softmax Function and Its Derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)  
  Eli Bendersky, 2016  
  `[bendersky2016-softmax]`

- [Understanding Transformers and GPT: An In-depth Overview](https://incml.github.io/2023/03/05/Transformer-GPT.html)  
  Hui Jiang, 2023  
  `[jiang2023-transformer-gpt]`

- [Xavier and Kaiming Initialization](https://pouannes.github.io/blog/initialization/)  
  Pierre Ouannes, 2019  
  `[ouannes2019-init]`

- [Regularizing Your Neural Networks](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html)  
  Kian Katanforoosh & Daniel Kunin, 2018  
  `[katanforoosh2018-regularization]`

## Wikipedia

- [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) `[wiki-normal]`
- [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) `[wiki-chain-rule]`
- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) `[wiki-logreg]`
- [One-hot](https://en.wikipedia.org/wiki/One-hot) `[wiki-onehot]`
- [Sigmoid Function](https://en.wikipedia.org/wiki/Sigmoid_function) `[wiki-sigmoid]`
- [IEEE 754 (Floating-point Precision)](https://en.wikipedia.org/wiki/IEEE_754) `[wiki-ieee754]`

## Textbooks

- **Calculus**
  - [Keisler, H.: Elementary Calculus](https://people.math.wisc.edu/~hkeisler) `[keisler-calculus]`
  - [Whitman, D.: Multivariable Calculus](https://www.whitman.edu/mathematics/multivariable) `[whitman-multivariable]`
- **Deep Learning**
  - [Goodfellow, I. et al.: Deep Learning](https://www.deeplearningbook.org) `[goodfellow2016-dlbook]`
  - [Understanding Deep Learning](https://udlbook.github.io/udlbook/) `[udlbook2020]`
- **Discrete Mathematics**
  - [OpenDiscrete (Open Math Books)](https://discrete.openmathbooks.org/dmoi3.html) `[opendiscrete2023]`
- **Linear Algebra**
  - [Understanding Linear Algebra (Paul Penfield)](https://understandinglinearalgebra.org) `[ula2020]`
  - [Springer: Linear Algebra](https://link.springer.com/book/10.1007/978-3-031-41026-0) `[springer-la2023]`
- **Statistics**
  - [Hastie et al.: Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/) `[hastie2009-elemstat]`
  - [Probability Course](https://www.probabilitycourse.com/) `[probabilitycourse2023]`
- **Digital Signal Processing**
  - [Digital Signals Theory (Brian McFee)](https://brianmcfee.net/) `[mcfee-2023]`
  - [Smith, S.: Scientist & Engineerâ€™s Guide to DSP](https://www.analog.com/en/resources/technical-books/scientist_engineers_guide.html) `[smith-dsp]`

## Source Code

- [adriancable/qwen3.c](https://github.com/adriancable/qwen3.c) `[adriancable-qwen3c]`

## Open Educational Resources (OER) / Creative Commons

- [OpenStax: Math Textbooks](https://openstax.org/subjects/math) `[openstax-math]`
- [Open Textbook Library: Mathematics (UMN)](https://open.umn.edu/opentextbooks/subjects/mathematics) `[umn-math-open]`

## Notes / TODO

- **Reference only**: public domain, creative commons, or FOSS-licensed materials.
- **Avoid**: proprietary, patented, or otherwise restricted materials. This is easier said than done.
- Expand with subcategories (_Tokenization_, _Initialization_, _Regularization_) as needed.
- Add author/year for lookup.
- Optionally map `[citation-key]` to usage in code/docs.
- Maintain deduplication and alphabetization.
- Digital signal processing is a subtopic of mathematics and engineering, not a standalone field for citation purposes. Kept here for thematic relevance.
